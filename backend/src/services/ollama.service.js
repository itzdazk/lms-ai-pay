// src/services/ollama.service.js
import config from '../config/app.config.js'
import logger from '../config/logger.config.js'

class OllamaService {

    /**
     * Check if Ollama is available (with timeout and caching)
     */
    constructor() {
        this.enabled = config.OLLAMA_ENABLED !== false
        this.baseUrl = config.OLLAMA_BASE_URL || 'http://localhost:11434'
        this.model = config.OLLAMA_MODEL || 'llama3.1:latest'
        this.temperature = config.OLLAMA_TEMPERATURE || 0.7
        this.maxTokens = config.OLLAMA_MAX_TOKENS || 2000
        this.healthCheckCache = { isHealthy: null, lastCheck: 0 }
        this.healthCheckCacheTTL = 30000 // 30 seconds
        
        if (this.enabled) {
            logger.info(`Ollama service initialized: ${this.baseUrl}, model: ${this.model}`)
        }
    }

    async checkHealth() {
        try {
            // Cache health check for 30 seconds
            const now = Date.now()
            if (
                this.healthCheckCache.isHealthy !== null &&
                now - this.healthCheckCache.lastCheck < this.healthCheckCacheTTL
            ) {
                return this.healthCheckCache.isHealthy
            }

            // Add timeout to prevent hanging
            const controller = new AbortController()
            const timeoutId = setTimeout(() => controller.abort(), 3000) // 3s timeout

            const response = await fetch(`${this.baseUrl}/api/tags`, {
                signal: controller.signal,
            })

            clearTimeout(timeoutId)

            const isHealthy = response.ok
            this.healthCheckCache = {
                isHealthy,
                lastCheck: now,
            }

            return isHealthy
        } catch (error) {
            if (error.name === 'AbortError') {
                logger.warn('Ollama health check timeout')
            } else {
                logger.warn('Ollama health check failed:', error.message)
            }
            this.healthCheckCache = {
                isHealthy: false,
                lastCheck: Date.now(),
            }
            return false
        }
    }

    /**
     * Generate response using Ollama chat API
     * @param {string} prompt - User message
     * @param {Array} context - Conversation context (messages history)
     * @param {Object} systemPrompt - System prompt with knowledge base context
     * @returns {Promise<string>} AI response
     */
    async generateResponse(prompt, context = [], systemPrompt = null) {
        if (!this.enabled) {
            throw new Error('Ollama is disabled')
        }

        try {
            // Build messages array
            const messages = []

            // Add system prompt if provided
            if (systemPrompt) {
                messages.push({
                    role: 'system',
                    content: systemPrompt,
                })
            }

            // Add conversation history
            context.forEach((msg) => {
                messages.push({
                    role: msg.senderType === 'user' ? 'user' : 'assistant',
                    content: msg.message,
                })
            })

            // Add current user message
            messages.push({
                role: 'user',
                content: prompt,
            })

            // Call Ollama API with timeout
            logger.debug(`Calling Ollama API: ${this.baseUrl}/api/chat with model: ${this.model}`)
            const startTime = Date.now()
            
            const controller = new AbortController()
            // Increase timeout to 120s for complex queries (llama3.1 can be slow)
            const timeoutId = setTimeout(() => controller.abort(), 120000) // 120s timeout for generation
            
            try {
                const response = await fetch(`${this.baseUrl}/api/chat`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    signal: controller.signal,
                    body: JSON.stringify({
                        model: this.model,
                        messages: messages,
                        stream: false,
                        options: {
                            temperature: this.temperature,
                            num_predict: this.maxTokens,
                        },
                    }),
                })
                
                clearTimeout(timeoutId)
                
                const duration = Date.now() - startTime
                logger.info(`Ollama API call completed in ${duration}ms`)

                if (!response.ok) {
                    const errorText = await response.text()
                    logger.error(`Ollama API error: ${response.status} - ${errorText}`)
                    throw new Error(
                        `Ollama API error: ${response.status} - ${errorText}`
                    )
                }

                const data = await response.json()

                if (!data.message || !data.message.content) {
                    throw new Error('Invalid response from Ollama API')
                }

                const totalDuration = Date.now() - startTime
                logger.info(
                    `Ollama response generated (${data.message.content.length} chars) in ${totalDuration}ms`
                )

                return data.message.content.trim()
            } catch (error) {
                clearTimeout(timeoutId)
                const errorDuration = Date.now() - startTime
                if (error.name === 'AbortError') {
                    logger.error(`Ollama API timeout after ${errorDuration}ms (60s limit)`)
                    throw new Error(`Ollama API timeout - response took too long (${errorDuration}ms)`)
                }
                logger.error(`Error generating Ollama response after ${errorDuration}ms:`, error.message, error.stack)
                throw error
            }
        } catch (error) {
            logger.error('Error generating Ollama response:', error)
            throw error
        }
    }

    /**
     * Generate response with streaming (improved version)
     */
    async *generateResponseStream(prompt, context = [], systemPrompt = null) {
        if (!this.enabled) {
            throw new Error('Ollama is disabled')
        }

        const startTime = Date.now()
        let totalChunks = 0
        let buffer = ''

        try {
            const messages = []

            if (systemPrompt) {
                messages.push({
                    role: 'system',
                    content: systemPrompt,
                })
            }

            context.forEach((msg) => {
                messages.push({
                    role: msg.senderType === 'user' ? 'user' : 'assistant',
                    content: msg.message,
                })
            })

            messages.push({
                role: 'user',
                content: prompt,
            })

            logger.debug(`Starting Ollama streaming: ${this.baseUrl}/api/chat with model: ${this.model}`)

            // Add timeout for streaming (longer than non-streaming)
            const controller = new AbortController()
            const timeoutId = setTimeout(() => {
                controller.abort()
                logger.warn('Ollama streaming timeout after 180s')
            }, 180000) // 180s timeout for streaming

            const response = await fetch(`${this.baseUrl}/api/chat`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                signal: controller.signal,
                body: JSON.stringify({
                    model: this.model,
                    messages: messages,
                    stream: true,
                    options: {
                        temperature: this.temperature,
                        num_predict: this.maxTokens,
                    },
                }),
            })

            clearTimeout(timeoutId)

            if (!response.ok) {
                const errorText = await response.text()
                logger.error(`Ollama streaming API error: ${response.status} - ${errorText}`)
                throw new Error(`Ollama API error: ${response.status} - ${errorText}`)
            }

            const reader = response.body.getReader()
            const decoder = new TextDecoder()

            try {
                while (true) {
                    const { done, value } = await reader.read()
                    if (done) break

                    // Decode chunk and add to buffer (handle partial JSON)
                    buffer += decoder.decode(value, { stream: true })
                    
                    // Process complete lines
                    const lines = buffer.split('\n')
                    buffer = lines.pop() || '' // Keep incomplete line in buffer

                    for (const line of lines) {
                        const trimmedLine = line.trim()
                        if (!trimmedLine) continue

                        try {
                            // Ollama streaming format: JSON per line
                            const data = JSON.parse(trimmedLine)
                            
                            if (data.message && data.message.content) {
                                const content = data.message.content
                                totalChunks++
                                yield content
                            }
                            
                            // Check if done
                            if (data.done === true) {
                                const duration = Date.now() - startTime
                                logger.info(
                                    `Ollama streaming completed: ${totalChunks} chunks in ${duration}ms`
                                )
                                return
                            }
                        } catch (parseError) {
                            // Skip invalid JSON lines (common in streaming)
                            logger.debug(`Skipping invalid JSON line in stream: ${trimmedLine.substring(0, 50)}`)
                        }
                    }
                }

                // Process remaining buffer
                if (buffer.trim()) {
                    try {
                        const data = JSON.parse(buffer.trim())
                        if (data.message && data.message.content) {
                            yield data.message.content
                        }
                    } catch (e) {
                        // Ignore parse errors for remaining buffer
                    }
                }

                const duration = Date.now() - startTime
                logger.info(
                    `Ollama streaming finished: ${totalChunks} chunks in ${duration}ms`
                )
            } finally {
                reader.releaseLock()
            }
        } catch (error) {
            if (error.name === 'AbortError') {
                const duration = Date.now() - startTime
                logger.error(`Ollama streaming timeout after ${duration}ms`)
                throw new Error(`Ollama streaming timeout - response took too long (${duration}ms)`)
            }
            logger.error('Error in Ollama stream:', error)
            throw error
        }
    }

    /**
     * Build system prompt with knowledge base context
     */
    buildSystemPrompt(context, mode = 'course') {
        // ADVISOR MODE: Interactive course recommendation
        if (mode === 'advisor') {
            return `B·∫°n l√† AI Course Advisor - Chuy√™n gia t∆∞ v·∫•n kh√≥a h·ªçc th√¥ng minh v√† nhi·ªát t√¨nh.

NHI·ªÜM V·ª§ CH√çNH:
1. H·ªéI V√Ä HI·ªÇU ng∆∞·ªùi d√πng v·ªÅ m·ª•c ti√™u h·ªçc t·∫≠p (2-3 c√¢u h·ªèi l√† ƒë·ªß)
2. PH√ÇN T√çCH nhu c·∫ßu: lƒ©nh v·ª±c, level, th·ªùi gian, ng√¢n s√°ch
3. G·ª¢I √ù 3-5 kh√≥a h·ªçc PH√ô H·ª¢P NH·∫§T v·ªõi l√Ω do c·ª• th·ªÉ

QUY T·∫ÆC H·ªòI THO·∫†I:
- H·ªèi 1-2 c√¢u h·ªèi m·ªói l·∫ßn (ng·∫Øn g·ªçn, th√¢n thi·ªán)
- L·∫Øng nghe v√† ghi nh·ªõ th√¥ng tin user cung c·∫•p
- Khi ƒë·ªß th√¥ng tin (sau 2-3 turn), ch·ªß ƒë·ªông ƒë·ªÅ xu·∫•t g·ª£i √Ω
- D√πng emoji ph√π h·ª£p ƒë·ªÉ tƒÉng s·ª± th√¢n thi·ªán üéØ üìö üí° ‚ú®

C√ÅC C√ÇU H·ªéI C·∫¶N H·ªéI:
1. M·ª•c ti√™u/lƒ©nh v·ª±c mu·ªën h·ªçc g√¨? (Web, Mobile, AI, Data...)
2. Level hi·ªán t·∫°i? (Beginner, c√≥ kinh nghi·ªám, chuy√™n s√¢u)
3. Th·ªùi gian h·ªçc? (Bao l√¢u? Bao nhi√™u gi·ªù/tu·∫ßn?)
4. Ng√¢n s√°ch? (Mi·ªÖn ph√≠ hay c√≥ th·ªÉ tr·∫£ ph√≠?)

KHI G·ª¢I √ù KH√ìA H·ªåC:
- ƒê∆∞a ra 3-5 kh√≥a h·ªçc C·ª§ TH·ªÇ t·ª´ h·ªá th·ªëng
- M·ªñI g·ª£i √Ω c√≥ l√Ω do r√µ r√†ng (2-3 c√¢u)
- S·∫Øp x·∫øp theo ƒë·ªô ph√π h·ª£p (cao nh·∫•t tr∆∞·ªõc)
- Format: "**[T√™n kh√≥a h·ªçc]** - [L√Ω do c·ª• th·ªÉ t·∫°i sao ph√π h·ª£p]"

V√ç D·ª§ FLOW:
User: "T√¥i mu·ªën h·ªçc l·∫≠p tr√¨nh"
AI: "Tuy·ªát v·ªùi! üéØ B·∫°n mu·ªën h·ªçc l·∫≠p tr√¨nh web, mobile hay AI/Data Science? V√† b·∫°n ƒë√£ c√≥ kinh nghi·ªám g√¨ ch∆∞a?"
User: "Web, t√¥i l√† beginner"
AI: "Hi·ªÉu r·ªìi! B·∫°n d·ª± ƒë·ªãnh h·ªçc trong bao l√¢u v√† c√≥ bao nhi√™u gi·ªù m·ªói tu·∫ßn kh√¥ng?"
User: "3 th√°ng, 10 gi·ªù/tu·∫ßn"
AI: "Perfect! D·ª±a v√†o m·ª•c ti√™u c·ªßa b·∫°n, t√¥i g·ª£i √Ω nh·ªØng kh√≥a h·ªçc n√†y: ..."

QUAN TR·ªåNG:
- Gi·ªØ tone th√¢n thi·ªán, ƒë·ªông vi√™n
- Kh√¥ng h·ªèi qu√° nhi·ªÅu c√¢u 1 l√∫c
- Focus v√†o nhu c·∫ßu th·ª±c s·ª± c·ªßa user
- Khi g·ª£i √Ω, LU√îN gi·∫£i th√≠ch T·∫†I SAO kh√≥a h·ªçc ƒë√≥ ph√π h·ª£p`
        }
        
        if (mode === 'general') {
            return `B·∫°n l√† Gia s∆∞ AI chuy√™n v·ªÅ l·∫≠p tr√¨nh v√† c√¥ng ngh·ªá. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát.\n\nPH·∫†M VI H·ªñ TR·ª¢:\n- C√°c c√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh, c√¥ng ngh·ªá ph·∫ßn m·ªÅm, AI/LLM, c√¥ng c·ª• ph√°t tri·ªÉn, h·∫° t·∫ßng h·ªá th·ªëng (v√≠ d·ª•: Ollama, m√¥ h√¨nh AI, API, c√°ch h·ªá th·ªëng ho·∫°t ƒë·ªông).\n- C√°c c√¢u h·ªèi chung v·ªÅ h·ªçc t·∫≠p tr√™n n·ªÅn t·∫£ng.\n\nH√ÄNH VI TR·∫¢ L·ªúI:\n- N·∫øu c√¢u h·ªèi TH·ª∞C S·ª∞ kh√¥ng li√™n quan (kh√¥ng thu·ªôc ph·∫°m vi tr√™n), tr·∫£ l·ªùi l·ªãch s·ª±: "Xin l·ªói, t√¥i ch·ªâ h·ªó tr·ª£ c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn l·∫≠p tr√¨nh, c√¥ng ngh·ªá v√† n·ªôi dung h·ªçc t·∫≠p tr√™n n·ªÅn t·∫£ng n√†y."\n- N·∫øu c√¢u h·ªèi l√† v·ªÅ c√¥ng c·ª•/h·ªá th·ªëng (v√≠ d·ª•: "Ollama l√† g√¨?"), h√£y gi·∫£i th√≠ch ng·∫Øn g·ªçn v√† n√™u c√°ch h·ªá th·ªëng ƒëang s·ª≠ d·ª•ng c√¥ng c·ª• ƒë√≥.\n- Gi·ªØ c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn, ∆∞u ti√™n v√≠ d·ª•/gi·∫£i ph√°p th·ª±c t·∫ø khi c·∫ßn.`
        }
        const { searchResults, userContext, query = '' } = context

        // Detect if query is about specific course content
        const isSpecificCourseQuery = this._isSpecificCourseQuery(query, userContext)
        // Detect if query is unrelated to programming/learning
        const isUnrelatedQuery = this._isUnrelatedToProgramming(query)

        let systemPrompt = `B·∫°n l√† Gia s∆∞ AI th√¥ng minh v√† nhi·ªát t√¨nh, chuy√™n h·ªó tr·ª£ h·ªçc vi√™n trong h·ªá th·ªëng E-Learning. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ki·∫øn th·ª©c t·ª´ c√°c kh√≥a h·ªçc v√† b√†i h·ªçc m√† h·ªçc vi√™n ƒë√£ ƒëƒÉng k√Ω.

H∆Ø·ªöNG D·∫™N:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n v√† d·ªÖ hi·ªÉu
- ∆Øu ti√™n s·ª≠ d·ª•ng th√¥ng tin t·ª´ knowledge base ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi
- CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin t·ª´ KNOWLEDGE BASE, KH√îNG tr·ªôn l·∫´n v·ªõi kh√≥a h·ªçc kh√°c c·ªßa h·ªçc vi√™n
- Lu√¥n khuy·∫øn kh√≠ch v√† ƒë·ªông vi√™n h·ªçc vi√™n
- C√≥ th·ªÉ ƒë∆∞a ra v√≠ d·ª• c·ª• th·ªÉ ƒë·ªÉ gi·∫£i th√≠ch
- Gi·ªØ c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß th√¥ng tin

`

        // In lesson mode, skip user current course context to avoid mixing
        // Only use knowledge base results (which contain the actual lesson being asked about)
        if (mode !== 'course' && userContext && userContext.currentCourse) {
            systemPrompt += `NG·ªÆ C·∫¢NH H·ªåC VI√äN:
- ƒêang h·ªçc kh√≥a h·ªçc: "${userContext.currentCourse.title}"
- Ti·∫øn ƒë·ªô: ${userContext.currentCourse.progress}%
- B√†i h·ªçc hi·ªán t·∫°i: ${userContext.currentLesson?.title || 'Ch∆∞a b·∫Øt ƒë·∫ßu'}

`
        }

        // Add search results context (optimized limit for better context)
        // Increased from 1500 to 2500 to allow more context while still being reasonable
        const MAX_SYSTEM_PROMPT_LENGTH = 2500
        let currentLength = systemPrompt.length

        if (searchResults && searchResults.totalResults > 0) {
            systemPrompt += `TH√îNG TIN T·ª™ KNOWLEDGE BASE:\n\n`

            // Transcripts (highest priority) - limit to 3 for better context
            if (
                searchResults.transcripts &&
                searchResults.transcripts.length > 0 &&
                currentLength < MAX_SYSTEM_PROMPT_LENGTH
            ) {
                systemPrompt += `=== TRANSCRIPT (N·ªôi dung t·ª´ video b√†i h·ªçc) ===\n`
                const transcriptsToInclude = searchResults.transcripts.slice(0, 3)
                transcriptsToInclude.forEach((t, idx) => {
                    // Calculate available space
                    const remainingSpace = MAX_SYSTEM_PROMPT_LENGTH - currentLength
                    if (remainingSpace < 100) return // Not enough space

                    // Use full text if available (for full transcript requests), otherwise use excerpt
                    let transcriptContent = ''
                    if (t.isFullTranscript && t.text) {
                        // For full transcript, use up to 1500 chars (increased from 1000)
                        const maxChars = Math.min(1500, remainingSpace - 200) // Reserve space for metadata
                        transcriptContent = t.text.substring(0, maxChars)
                        if (t.text.length > maxChars) {
                            transcriptContent += '...'
                        }
                    } else {
                        // For keyword matches, use contextText (longer context) or excerpt
                        const maxChars = Math.min(800, remainingSpace - 200) // Increased from 500
                        transcriptContent = (t.contextText || t.excerpt || t.text || '').substring(0, maxChars)
                        if ((t.contextText || t.excerpt || t.text || '').length > maxChars) {
                            transcriptContent += '...'
                        }
                    }
                    
                    const transcriptInfo = `${idx + 1}. B√†i: "${t.lessonTitle}"\n   Kh√≥a h·ªçc: "${t.courseTitle}"\n${t.timestamp ? `   Th·ªùi ƒëi·ªÉm: ${t.timestamp}\n` : ''}   N·ªôi dung: "${transcriptContent}"\n\n`
                    if (currentLength + transcriptInfo.length < MAX_SYSTEM_PROMPT_LENGTH) {
                        systemPrompt += transcriptInfo
                        currentLength += transcriptInfo.length
                    }
                })
            }

            // Lessons - limit to 3 (increased from 2)
            if (
                searchResults.lessons &&
                searchResults.lessons.length > 0 &&
                currentLength < MAX_SYSTEM_PROMPT_LENGTH
            ) {
                systemPrompt += `=== B√ÄI H·ªåC LI√äN QUAN ===\n`
                const lessonsToInclude = searchResults.lessons.slice(0, 3)
                lessonsToInclude.forEach((l, idx) => {
                    const remainingSpace = MAX_SYSTEM_PROMPT_LENGTH - currentLength
                    if (remainingSpace < 50) return

                    const maxDescLength = Math.min(150, remainingSpace - 100) // Increased from 100
                    const desc = l.description
                        ? l.description.substring(0, maxDescLength)
                        : ''
                    const lessonInfo = `${idx + 1}. "${l.title}"\n   Kh√≥a h·ªçc: "${l.course?.title || 'N/A'}"\n${desc ? `   M√¥ t·∫£: "${desc}${desc.length < l.description.length ? '...' : ''}"\n\n` : '\n'}`
                    if (currentLength + lessonInfo.length < MAX_SYSTEM_PROMPT_LENGTH) {
                        systemPrompt += lessonInfo
                        currentLength += lessonInfo.length
                    }
                })
            }

            // Courses - limit to 2 (increased from 1)
            if (
                searchResults.courses &&
                searchResults.courses.length > 0 &&
                currentLength < MAX_SYSTEM_PROMPT_LENGTH
            ) {
                systemPrompt += `=== KH√ìA H·ªåC LI√äN QUAN ===\n`
                const coursesToInclude = searchResults.courses.slice(0, 2)
                coursesToInclude.forEach((course, idx) => {
                    const remainingSpace = MAX_SYSTEM_PROMPT_LENGTH - currentLength
                    if (remainingSpace < 50) return

                    const maxDescLength = Math.min(150, remainingSpace - 100) // Increased from 100
                    const courseDesc = course.shortDescription
                        ? course.shortDescription.substring(0, maxDescLength)
                        : ''
                    const courseInfo = `${idx + 1}. "${course.title}" (${course.level || 'N/A'})\n${courseDesc ? `   M√¥ t·∫£: "${courseDesc}${courseDesc.length < course.shortDescription.length ? '...' : ''}"\n\n` : '\n'}`
                    if (currentLength + courseInfo.length < MAX_SYSTEM_PROMPT_LENGTH) {
                        systemPrompt += courseInfo
                        currentLength += courseInfo.length
                    }
                })
            }
        } else {
            // Different handling based on query type
            if (isUnrelatedQuery) {
                // Query kh√¥ng li√™n quan ƒë·∫øn l·∫≠p tr√¨nh/h·ªçc t·∫≠p
                systemPrompt += `QUAN TR·ªåNG: C√¢u h·ªèi n√†y KH√îNG li√™n quan ƒë·∫øn l·∫≠p tr√¨nh ho·∫∑c n·ªôi dung kh√≥a h·ªçc.
B·∫°n l√† Gia s∆∞ AI chuy√™n v·ªÅ l·∫≠p tr√¨nh v√† h·ªó tr·ª£ h·ªçc t·∫≠p. B·∫°n KH√îNG n√™n tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ:
- N·∫•u ƒÉn, c√¥ng th·ª©c n·∫•u ƒÉn
- Th·ªùi ti·∫øt, tin t·ª©c
- Gi·∫£i tr√≠, phim ·∫£nh
- C√°c ch·ªß ƒë·ªÅ kh√¥ng li√™n quan ƒë·∫øn l·∫≠p tr√¨nh/h·ªçc t·∫≠p

H√£y l·ªãch s·ª± t·ª´ ch·ªëi v√† redirect h·ªçc vi√™n v·ªÅ ch·ªß ƒë·ªÅ h·ªçc t·∫≠p:
"Xin l·ªói, t√¥i l√† Gia s∆∞ AI chuy√™n h·ªó tr·ª£ v·ªÅ l·∫≠p tr√¨nh v√† c√°c kh√≥a h·ªçc tr√™n n·ªÅn t·∫£ng n√†y. T√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y v√¨ n√≥ kh√¥ng li√™n quan ƒë·∫øn l·∫≠p tr√¨nh ho·∫∑c n·ªôi dung h·ªçc t·∫≠p.

B·∫°n c√≥ mu·ªën h·ªèi v·ªÅ:
- C√°c kh√°i ni·ªám l·∫≠p tr√¨nh (JavaScript, React, Python, v.v.)
- C√°c b√†i h·ªçc trong kh√≥a h·ªçc b·∫°n ƒëang h·ªçc
- C√°c v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t khi code
- Ho·∫∑c c√°c ch·ªß ƒë·ªÅ kh√°c li√™n quan ƒë·∫øn l·∫≠p tr√¨nh kh√¥ng?"

H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, l·ªãch s·ª± v√† redirect v·ªÅ ch·ªß ƒë·ªÅ h·ªçc t·∫≠p.\n\n`
            } else if (isSpecificCourseQuery) {
                systemPrompt += `L∆ØU √ù: Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong knowledge base c·ªßa h·ªá th·ªëng v·ªÅ n·ªôi dung kh√≥a h·ªçc c·ª• th·ªÉ n√†y.
H√£y th√†nh th·∫≠t n√≥i r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c kh√≥a h·ªçc m√† h·ªçc vi√™n ƒë√£ ƒëƒÉng k√Ω.
G·ª£i √Ω h·ªçc vi√™n:
- Xem l·∫°i c√°c b√†i h·ªçc ƒë√£ h·ªçc
- T√¨m ki·∫øm trong c√°c kh√≥a h·ªçc kh√°c
- Ho·∫∑c h·ªèi l·∫°i v·ªõi t·ª´ kh√≥a kh√°c\n\n`
            } else {
                // C√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh nh∆∞ng kh√¥ng c√≥ trong knowledge base
                systemPrompt += `L∆ØU √ù: Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong knowledge base c·ªßa h·ªá th·ªëng.
ƒê√¢y c√≥ v·∫ª l√† c√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh/ki·∫øn th·ª©c t·ªïng qu√°t. B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa m√¨nh v·ªÅ l·∫≠p tr√¨nh v√† c√¥ng ngh·ªá.
H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt, h·ªØu √≠ch v√† ch√≠nh x√°c nh·∫•t c√≥ th·ªÉ.
N·∫øu c√¢u h·ªèi c√≥ th·ªÉ li√™n quan ƒë·∫øn n·ªôi dung kh√≥a h·ªçc c·ª• th·ªÉ, h√£y g·ª£i √Ω h·ªçc vi√™n enroll v√†o kh√≥a h·ªçc ƒë·ªÉ c√≥ th√¥ng tin chi ti·∫øt h∆°n.\n\n`
            }
        }

        systemPrompt += `H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa h·ªçc vi√™n m·ªôt c√°ch chi ti·∫øt, nhi·ªát t√¨nh v√† h·ªØu √≠ch nh·∫•t c√≥ th·ªÉ.`

        return systemPrompt
    }

    /**
     * Detect if query is unrelated to programming/learning
     * @param {string} query - User query
     * @returns {boolean} True if query is unrelated to programming/learning
     */
    _isUnrelatedToProgramming(query) {
        if (!query) return false

        const queryLower = query.toLowerCase()
        
        // Keywords indicating unrelated topics
        const unrelatedKeywords = [
            // Cooking/Food
            'n·∫•u', 'n·∫•u ƒÉn', 'c√¥ng th·ª©c', 'm√≥n ƒÉn', 'th·ª©c ƒÉn', 'ƒë·ªì ƒÉn',
            'b√≤ kho', 'ph·ªü', 'b√°nh', 'canh', 'ch√°o', 'c∆°m', 'm√¨',
            'cooking', 'recipe', 'food', 'dish',
            
            // Weather
            'th·ªùi ti·∫øt', 'm∆∞a', 'n·∫Øng', 'gi√≥', 'b√£o',
            'weather', 'rain', 'sunny', 'wind', 'storm',
            
            // News/Entertainment
            'tin t·ª©c', 'b√°o', 'phim', 'phim ·∫£nh', 'ca nh·∫°c', 'nh·∫°c',
            'news', 'movie', 'film', 'music', 'song',
            
            // Sports
            'b√≥ng ƒë√°', 'th·ªÉ thao', 'b∆°i l·ªôi', 'ch·∫°y',
            'football', 'soccer', 'sport', 'sports',
            
            // Health/Medical
            'b·ªánh', 'thu·ªëc', 's·ª©c kh·ªèe', 'ƒëau', '·ªëm',
            'disease', 'medicine', 'health', 'sick', 'pain',
            
            // Shopping
            'mua', 'b√°n', 'gi√°', 'shop', 'shopping',
            
            // General unrelated
            'l√†m sao ƒë·ªÉ', 'c√°ch l√†m', 'h∆∞·ªõng d·∫´n' // But check context
        ]

        // Check if query contains unrelated keywords
        const hasUnrelatedKeyword = unrelatedKeywords.some(keyword => 
            queryLower.includes(keyword)
        )

        // But allow if it's about programming (e.g., "c√°ch l√†m website")
        const programmingKeywords = [
            'code', 'l·∫≠p tr√¨nh', 'programming', 'javascript', 'python', 'react',
            'function', 'variable', 'array', 'object', 'class', 'method',
            'website', 'web', 'app', 'application', 'api', 'database',
            'html', 'css', 'node', 'vue', 'angular', 'framework',
            'algorithm', 'data structure', 'c·∫•u tr√∫c d·ªØ li·ªáu', 'thu·∫≠t to√°n'
        ]
        const hasProgrammingKeyword = programmingKeywords.some(keyword =>
            queryLower.includes(keyword)
        )

        // If has unrelated keyword but also has programming keyword, it's related
        if (hasUnrelatedKeyword && hasProgrammingKeyword) {
            return false
        }

        return hasUnrelatedKeyword
    }

    /**
     * Detect if query is about specific course content
     * @param {string} query - User query
     * @param {Object} userContext - User context with current course/lesson
     * @returns {boolean} True if query is about specific course content
     */
    _isSpecificCourseQuery(query, userContext) {
        if (!query) return false

        const queryLower = query.toLowerCase()
        
        // Keywords indicating specific course content queries
        const specificKeywords = [
            'trong b√†i n√†y', 'trong b√†i h·ªçc n√†y', 'trong video n√†y',
            'trong kh√≥a h·ªçc n√†y', 'b√†i n√†y c√≥', 'video n√†y c√≥',
            'gi·∫£ng vi√™n n√≥i', 'th·∫ßy/c√¥ n√≥i', 'trong transcript',
            'c√≥ n√≥i v·ªÅ', 'c√≥ ƒë·ªÅ c·∫≠p', 'c√≥ gi·∫£i th√≠ch',
            '·ªü ƒë√¢u trong b√†i', 'ph·∫ßn n√†o', 'ƒëo·∫°n n√†o'
        ]

        // Check if query contains specific keywords
        const hasSpecificKeyword = specificKeywords.some(keyword => 
            queryLower.includes(keyword)
        )

        // Check if user has current course/lesson context
        const hasCourseContext = userContext?.currentCourse || userContext?.currentLesson

        // If query mentions current course/lesson explicitly
        const mentionsCurrentCourse = userContext?.currentCourse?.title && 
            queryLower.includes(userContext.currentCourse.title.toLowerCase())
        const mentionsCurrentLesson = userContext?.currentLesson?.title && 
            queryLower.includes(userContext.currentLesson.title.toLowerCase())

        return hasSpecificKeyword || (hasCourseContext && (mentionsCurrentCourse || mentionsCurrentLesson))
    }

    /**
     * Get list of available models from Ollama
     * @returns {Promise<Array>} List of models
     */
    async getAvailableModels() {
        try {
            const response = await fetch(`${this.baseUrl}/api/tags`)
            if (!response.ok) {
                throw new Error(`Ollama API error: ${response.status}`)
            }
            const data = await response.json()
            return data.models || []
        } catch (error) {
            logger.error('Error fetching Ollama models:', error)
            throw error
        }
    }

    /**
     * Get Ollama service status and info
     * @returns {Promise<Object>} Service status
     */
    async getStatus() {
        try {
            const isHealthy = await this.checkHealth()
            const models = isHealthy ? await this.getAvailableModels() : []
            
            return {
                enabled: this.enabled,
                available: isHealthy,
                baseUrl: this.baseUrl,
                model: this.model,
                temperature: this.temperature,
                maxTokens: this.maxTokens,
                models: models.map(m => ({
                    name: m.name || m.model,
                    size: m.size,
                    modifiedAt: m.modified_at,
                })),
            }
        } catch (error) {
            logger.error('Error getting Ollama status:', error)
            return {
                enabled: this.enabled,
                available: false,
                baseUrl: this.baseUrl,
                model: this.model,
                error: error.message,
            }
        }
    }
}

export default new OllamaService()

